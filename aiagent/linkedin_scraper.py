# aiagent/linkedin_scraper.py
# âœ… LinkedIn å€™è£œè€…æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ãƒ»Cookieä¸ä½¿ç”¨ãƒ»ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œç‰ˆï¼‰
# âœ… æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã€ŒSIerã€ã«å¤‰æ›´æ¸ˆã¿

import os
import csv
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager

# ======================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
# ======================================
DATA_DIR = os.path.join(os.path.dirname(__file__), "../data")
OUTPUT_FILE = os.path.join(DATA_DIR, "candidates_raw.csv")

# ======================================
# æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³
# ======================================
def linkedin_manual_login():
    print("ğŸ”‘ æ‰‹å‹•ãƒ­ã‚°ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¾ã™...")
    options = Options()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    options.add_experimental_option("detach", True)

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.get("https://www.linkedin.com/login")

    print("ğŸŒ ã”è‡ªèº«ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§LinkedInã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")
    print("ğŸ’¡ ãƒ­ã‚°ã‚¤ãƒ³å¾Œã€feedãƒšãƒ¼ã‚¸ï¼ˆhttps://www.linkedin.com/feed/ï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™...")

    while True:
        try:
            if "feed" in driver.current_url:
                print("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
                break
        except Exception:
            pass
        time.sleep(2)
    return driver

# ======================================
# ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æŠ½å‡ºé–¢æ•°
# ======================================
def extract_profiles(driver):
    """ç¾åœ¨è¡¨ç¤ºä¸­ã®æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚’æŠ½å‡º"""
    script = """
    function getAllProfileLinks() {
        const results = [];
        const anchors = Array.from(document.querySelectorAll('a[href*="/in/"]'));
        const seen = new Set();
        for (const a of anchors) {
            const url = (a.href || '').split('?')[0];
            if (!url || seen.has(url)) continue;
            seen.add(url);
            const container = a.closest("li") || a.closest("div");
            const nameEl = container ? (container.querySelector("span[dir='ltr'], span[aria-hidden='true']") || a) : a;
            const name = (nameEl.textContent || '').trim();
            if (!name) continue;
            results.push({ name, url });
        }
        return results;
    }
    return getAllProfileLinks();
    """
    return driver.execute_script(script) or []

# ======================================
# ãƒšãƒ¼ã‚¸ã”ã¨ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
# ======================================
def scroll_page(driver):
    last_h = 0
    for _ in range(6):
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(1.8)
        h = driver.execute_script("return document.body.scrollHeight")
        if h == last_h:
            break
        last_h = h

# ======================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆ3ãƒšãƒ¼ã‚¸åˆ†æŠ½å‡ºï¼‰
# ======================================
def scrape_candidates():
    os.makedirs(DATA_DIR, exist_ok=True)
    driver = linkedin_manual_login()

    # ğŸ” æ¤œç´¢æ¡ä»¶ã‚’ã€ŒSIerã€ã«å›ºå®š
    keywords = "SIer"
    search_url = f"https://www.linkedin.com/search/results/people/?keywords={keywords}&origin=GLOBAL_SEARCH_HEADER"

    print(f"ğŸ” æ¤œç´¢URL: {search_url}")
    driver.get(search_url)
    wait = WebDriverWait(driver, 20)
    try:
        wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
    except TimeoutException:
        pass

    all_rows = []
    seen_urls = set()
    MAX_PAGES = 3

    for page in range(1, MAX_PAGES + 1):
        print(f"\nğŸ“„ ãƒšãƒ¼ã‚¸ {page} ã®å€™è£œè€…ã‚’æŠ½å‡ºä¸­...")
        time.sleep(4)
        scroll_page(driver)
        time.sleep(2)

        data = extract_profiles(driver)
        print(f"ğŸ“¦ ãƒšãƒ¼ã‚¸ {page} ã®æŠ½å‡ºä»¶æ•°: {len(data)}")

        for item in data:
            name = (item.get("name") or "").strip()
            url = (item.get("url") or "").split("?")[0]
            if not name or not url or url in seen_urls:
                continue
            seen_urls.add(url)
            all_rows.append({"name": name, "url": url})

        # ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦æ¬¡ãƒšãƒ¼ã‚¸ã¸
        try:
            next_btn = driver.find_element(
                By.XPATH, "//button[contains(., 'æ¬¡ã¸')] | //button[contains(., 'Next')]"
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
            time.sleep(1)
            next_btn.click()
            print("â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸é·ç§»ã—ã¾ã™...")
            time.sleep(5)
        except NoSuchElementException:
            print("ğŸš« æ¬¡ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
            break

    # CSVå‡ºåŠ›
    print(f"\nğŸ“Š å…¨ãƒšãƒ¼ã‚¸åˆè¨ˆã®æœ‰åŠ¹å€™è£œè€…æ•°: {len(all_rows)}")
    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["name", "url"])
        writer.writeheader()
        writer.writerows(all_rows)

    print(f"ğŸ’¾ CSVä¿å­˜å®Œäº†: {OUTPUT_FILE}")
    print("âœ… å€™è£œè€…æŠ½å‡ºå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã¯é–‰ã˜ã¦ã‚‚OKã§ã™ã€‚")
    driver.quit()

# ======================================
# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ======================================
if __name__ == "__main__":
    scrape_candidates()
