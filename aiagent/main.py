# aiagent/main.py
import streamlit as st
import sys
import os
from datetime import datetime

# =====================================
# ãƒ‘ã‚¹è¨­å®šï¼ˆaiagent ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦èªè­˜ã•ã›ã‚‹ï¼‰
# =====================================
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# =====================================
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿
# =====================================
from aiagent.linkedin_scraper import scrape_candidates
from aiagent.analyzer import analyze_candidates
from aiagent.linkedin_sender import send_connection_requests
from dotenv import load_dotenv

# =====================================
# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
# =====================================
load_dotenv()

# =====================================
# Streamlit ãƒšãƒ¼ã‚¸è¨­å®š
# =====================================
st.set_page_config(page_title="LinkedIn Auto Agent", layout="wide")

# =====================================
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
# =====================================
if "active_tab" not in st.session_state:
    st.session_state.active_tab = "AIææ¡ˆæ¡ä»¶"
if "log_messages" not in st.session_state:
    st.session_state.log_messages = []
if "last_run" not in st.session_state:
    st.session_state.last_run = None

# =====================================
# ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹å®šç¾©
# =====================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "../data")
SENT_LOG_PATH = os.path.join(DATA_DIR, "sent_log.csv")
SCORED_PATH = os.path.join(DATA_DIR, "candidates_scored.csv")
RAW_PATH = os.path.join(DATA_DIR, "candidates_raw.csv")

# =====================================
# ãƒ­ã‚°é–¢æ•°
# =====================================
def log(message: str):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    msg = f"[{timestamp}] {message}"
    st.session_state.log_messages.append(msg)
    print(msg)

# =====================================
# ã‚¿ãƒ–æ§‹æˆ
# =====================================
tabs = ["AIææ¡ˆæ¡ä»¶", "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "è¿”ä¿¡ãƒ•ã‚©ãƒ­ãƒ¼", "è¨­å®šãƒ»ãƒ­ã‚°"]
selected_tab = st.sidebar.radio("ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„", tabs, index=tabs.index(st.session_state.active_tab))
st.session_state.active_tab = selected_tab

# =====================================
# ã‚¿ãƒ–â‘ ï¼šAIææ¡ˆæ¡ä»¶
# =====================================
if selected_tab == "AIææ¡ˆæ¡ä»¶":
    st.title("ğŸ¤– AIææ¡ˆæ¡ä»¶")
    st.write("å€™è£œè€…æ¤œç´¢ â†’ AIã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° â†’ è‡ªå‹•é€ä¿¡ ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")

    with st.form("ai_conditions_form"):
        col1, col2 = st.columns(2)
        with col1:
            target_region = st.text_input("åœ°åŸŸæ¡ä»¶ï¼ˆä¾‹ï¼šTokyo, Japanï¼‰", "Japan")
        with col2:
            max_pages = st.number_input("æ¤œç´¢ãƒšãƒ¼ã‚¸æ•°ï¼ˆ1ã€œ10ï¼‰", min_value=1, max_value=10, value=2, step=1)
        job_keywords = st.text_area("è·ç¨®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", "ITã‚³ãƒ³ã‚µãƒ«, SIer, ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢")

        run_all = st.form_submit_button("ğŸ” æ¤œç´¢ â†’ ğŸ§  ã‚¹ã‚³ã‚¢ â†’ âœ‰ï¸ é€ä¿¡ï¼ˆãƒ¯ãƒ³ãƒœã‚¿ãƒ³å®Ÿè¡Œï¼‰")

    if run_all:
        with st.spinner("LinkedIn æ¤œç´¢ã‚’å®Ÿè¡Œä¸­..."):
            kw = ",".join([k.strip() for k in job_keywords.split(",") if k.strip()])
            count = scrape_candidates(keywords=kw, location=target_region, max_pages=int(max_pages))
            log(f"æ¤œç´¢å®Œäº†ï¼š{count}ä»¶ æŠ½å‡º â†’ {RAW_PATH}")

        with st.spinner("AIã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­..."):
            send_targets = analyze_candidates(input_csv=RAW_PATH, output_csv=SCORED_PATH)
            log(f"AIã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Œäº†ï¼šé€ä¿¡å¯¾è±¡ {len(send_targets)} ä»¶ â†’ {SCORED_PATH}")

        with st.spinner("LinkedIn ã«æ¥ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­..."):
            send_connection_requests()
            log("æ¥ç¶šãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ãƒ•ãƒ­ãƒ¼ã‚’å®Œäº†")

        st.success("âœ… ãƒ¯ãƒ³ãƒœã‚¿ãƒ³å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        st.session_state.last_run = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# =====================================
# ã‚¿ãƒ–â‘¡ï¼šãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
# =====================================
elif selected_tab == "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰":
    st.title("ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.write("é€ä¿¡æ•°ã‚„è¿”ä¿¡ç‡ãªã©ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚")

    import pandas as pd
    if os.path.exists(SENT_LOG_PATH):
        df = pd.read_csv(SENT_LOG_PATH)
        if not df.empty:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
            df["day"] = df["date"].dt.date

            st.subheader("æ—¥åˆ¥ é€ä¿¡æ•°")
            day_counts = df.groupby("day")["profile_url"].count().reset_index(name="é€ä¿¡æ•°")
            st.dataframe(day_counts, use_container_width=True)

            st.subheader("ç›´è¿‘ã®é€ä¿¡å±¥æ­´ï¼ˆæœ€æ–°50ä»¶ï¼‰")
            st.dataframe(df.sort_values("date", ascending=False).head(50), use_container_width=True)
        else:
            st.info("é€ä¿¡ãƒ­ã‚°ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚AIææ¡ˆæ¡ä»¶ã‚¿ãƒ–ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        st.info("é€ä¿¡ãƒ­ã‚°ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚AIææ¡ˆæ¡ä»¶ã‚¿ãƒ–ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# =====================================
# ã‚¿ãƒ–â‘¢ï¼šè¿”ä¿¡ãƒ•ã‚©ãƒ­ãƒ¼ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
# =====================================
elif selected_tab == "è¿”ä¿¡ãƒ•ã‚©ãƒ­ãƒ¼":
    st.title("ğŸ“¬ è¿”ä¿¡ãƒ•ã‚©ãƒ­ãƒ¼")
    st.write("è¿”ä¿¡æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®çµ±åˆã¯æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºã§è¡Œã„ã¾ã™ã€‚")

# =====================================
# ã‚¿ãƒ–â‘£ï¼šè¨­å®šãƒ»ãƒ­ã‚°
# =====================================
elif selected_tab == "è¨­å®šãƒ»ãƒ­ã‚°":
    st.title("âš™ï¸ è¨­å®šãƒ»ãƒ­ã‚°")
    st.write("APIã‚­ãƒ¼ã‚„é€ä¿¡åˆ¶å¾¡è¨­å®šã€ãƒ­ã‚°ã®ç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚")

    st.subheader("å®Ÿè¡Œãƒ­ã‚°")
    for msg in st.session_state.log_messages[-200:]:
        st.text(msg)
    st.caption("ï¼ˆæœ€æ–°200ä»¶ã‚’è¡¨ç¤ºï¼‰")

    st.divider()
    st.write(f"æœ€çµ‚å®Ÿè¡Œ: {st.session_state.last_run or 'æœªå®Ÿè¡Œ'}")
